{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "720f6961",
   "metadata": {},
   "source": [
    "# Improving Large Language Model Reasoning Accuracy via Multi-Agent Optimization\n",
    "\n",
    "## VECTRA demo (Transformers-only)\n",
    "\n",
    "This notebook demonstrates the project thesis: **reasoning accuracy can be improved at inference-time** by using **multi-agent optimization** rather than a single-pass generation.\n",
    "\n",
    "We compare:\n",
    "- **Baseline**: one local Transformers generation per question\n",
    "- **VECTRA**: tool-loop + parallel attempts + early-stop voting (an optimization over multiple candidate trajectories)\n",
    "\n",
    "The benchmark uses 4 small reasoning suites and runs **without any OpenAI API calls**.\n",
    "\n",
    "## Constraints\n",
    "- Model is loaded locally via `transformers` with Hugging Face id: `openai/gpt-oss-20b`.\n",
    "- If you canâ€™t load a 20B model on your machine, set `MODEL_ID` to a smaller model to validate the methodology."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b9a611",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Install VECTRA with Transformers support. You must install an appropriate `torch` build for your machine (CPU or CUDA).\n",
    "\n",
    "This demo is intentionally **Transformers-only** so the results reflect inference-time optimization (multi-attempt orchestration + tool execution), not API/provider effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676dde02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap: use VECTRA directly from GitHub source when this notebook is not run inside the repo.\n",
    "from __future__ import annotations\n",
    "import importlib.util\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "REPO_GIT_URL = \"https://github.com/GokulPrasathM/VECTRA.git\"\n",
    "REPO_ZIP_URL = \"https://github.com/GokulPrasathM/VECTRA/archive/refs/heads/main.zip\"\n",
    "BRANCH = \"main\"\n",
    "CLONE_DIR = Path.cwd() / \"_vectra_repo\"\n",
    "\n",
    "def _pip_install(*args: str) -> None:\n",
    "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", *args]\n",
    "    print(\"Running:\", \" \".join(cmd))\n",
    "    subprocess.check_call(cmd)\n",
    "\n",
    "def _ensure_importable(module: str, pip_spec: str | None = None) -> None:\n",
    "    if importlib.util.find_spec(module) is not None:\n",
    "        return\n",
    "    _pip_install(pip_spec or module)\n",
    "\n",
    "def _ensure_repo_on_syspath() -> Path:\n",
    "    # If notebook is running from inside the repo already.\n",
    "    here = Path.cwd()\n",
    "    if (here / \"src\" / \"vectra\").exists():\n",
    "        repo_root = here\n",
    "    elif (CLONE_DIR / \"src\" / \"vectra\").exists():\n",
    "        repo_root = CLONE_DIR\n",
    "    else:\n",
    "        # Try git clone; fall back to zip download if git isn't available.\n",
    "        shutil.rmtree(CLONE_DIR, ignore_errors=True)\n",
    "        try:\n",
    "            subprocess.check_call([\"git\", \"--version\"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "            subprocess.check_call([\n",
    "                \"git\",\n",
    "                \"clone\",\n",
    "                \"--depth\",\n",
    "                \"1\",\n",
    "                \"--branch\",\n",
    "                BRANCH,\n",
    "                REPO_GIT_URL,\n",
    "                str(CLONE_DIR),\n",
    "            ])\n",
    "        except Exception:\n",
    "            import urllib.request\n",
    "            zip_path = here / \"_vectra_repo.zip\"\n",
    "            print(\"Downloading:\", REPO_ZIP_URL)\n",
    "            urllib.request.urlretrieve(REPO_ZIP_URL, zip_path)\n",
    "            with zipfile.ZipFile(zip_path) as zf:\n",
    "                zf.extractall(here)\n",
    "            extracted = here / f\"VECTRA-{BRANCH}\"\n",
    "            if extracted.exists():\n",
    "                extracted.rename(CLONE_DIR)\n",
    "            zip_path.unlink(missing_ok=True)\n",
    "        repo_root = CLONE_DIR\n",
    "    src_dir = repo_root / \"src\"\n",
    "    if str(src_dir) not in sys.path:\n",
    "        sys.path.insert(0, str(src_dir))\n",
    "    return repo_root\n",
    "\n",
    "# Ensure non-torch deps are present. (Torch install is environment-specific on Kaggle.)\n",
    "_ensure_importable(\"httpx\", \"httpx>=0.26\")\n",
    "_ensure_importable(\"datasets\")\n",
    "_ensure_importable(\"transformers\")\n",
    "_ensure_importable(\"accelerate\")\n",
    "repo_root = _ensure_repo_on_syspath()\n",
    "print(\"Using VECTRA source from:\", repo_root.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8a7282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies:\n",
    "# - This notebook now bootstraps VECTRA from GitHub source (see the cell above)\n",
    "# - You still need an appropriate `torch` build for your machine (CPU or CUDA).\n",
    "# If you're running locally and want to manage deps manually, you can use:\n",
    "# %pip install -q datasets transformers accelerate httpx\n",
    "# %pip install torch  # choose the right wheel/index-url for your environment\n",
    "import json\n",
    "import math\n",
    "import re\n",
    "import statistics\n",
    "import time\n",
    "from vectra import (\n",
    "    ScenarioSolveConfig,\n",
    "    TransformersClient,\n",
    "    TransformersClientConfig,\n",
    "    solve_scenario_async,\n",
    ")\n",
    "from vectra.scenario.attempts import AttemptConfig\n",
    "from vectra.tools.tool_loop import ToolLoopConfig\n",
    "from vectra.types import ChatMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058bcc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model + generation settings\n",
    "MODEL_ID = 'openai/gpt-oss-20b'\n",
    "TEMPERATURE = 0.2\n",
    "MAX_NEW_TOKENS = 256\n",
    "\n",
    "# Create ONE shared local Transformers client so we do not load 20B weights twice.\n",
    "client = TransformersClient(\n",
    "    TransformersClientConfig(\n",
    "        model_id=MODEL_ID,\n",
    "        max_new_tokens=MAX_NEW_TOKENS,\n",
    "        device_map='auto',\n",
    "        torch_dtype='auto',\n",
    "    )\n",
    ")\n",
    "\n",
    "print('Loaded Transformers model:', MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67994b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real reasoning benchmarks (sampled) via Hugging Face `datasets`\n",
    "#\n",
    "# Benchmarks used (commonly reported for reasoning evaluation):\n",
    "# - GSM8K (grade school math word problems)\n",
    "# - SVAMP (math word problems with simple arithmetic)\n",
    "# - MATH500 (subset of MATH; competition-style math problems)\n",
    "# - CommonsenseQA (multiple-choice commonsense reasoning)\n",
    "#\n",
    "# NOTE: This notebook still runs Transformers-only (no OpenAI API).\n",
    "# It may download datasets from Hugging Face the first time you run it.\n",
    "\n",
    "import random\n",
    "\n",
    "SAMPLES_PER_SUITE = 25  # increase for a more reliable estimate\n",
    "RANDOM_SEED = 7\n",
    "\n",
    "def _norm(s: str) -> str:\n",
    "    s = (s or '').strip()\n",
    "    s = re.sub(r'\\s+', ' ', s)\n",
    "    return s\n",
    "\n",
    "def _norm_compact(s: str) -> str:\n",
    "    s = (s or '').strip().lower()\n",
    "    s = re.sub(r'\\s+', '', s)\n",
    "    return s\n",
    "\n",
    "def extract_final(text: str) -> str:\n",
    "    if not text:\n",
    "        return ''\n",
    "    m = re.search(r'FINAL\\s*:\\s*(.+)$', text.strip(), flags=re.IGNORECASE)\n",
    "    return (m.group(1).strip() if m else text.strip())\n",
    "\n",
    "def _normalize_numeric(s: str) -> str:\n",
    "    s = _norm_compact(s).replace(',', '')\n",
    "    m = re.search(r'-?\\d+(?:\\.\\d+)?', s)\n",
    "    return m.group(0) if m else s\n",
    "\n",
    "def _normalize_choice_letter(s: str) -> str:\n",
    "    s2 = (s or '').strip().upper()\n",
    "    m = re.search(r'\\b([A-E])\\b', s2)\n",
    "    if m:\n",
    "        return m.group(1)\n",
    "    if s2[:1] in {'A','B','C','D','E'}:\n",
    "        return s2[:1]\n",
    "    return s2[:1]\n",
    "\n",
    "def is_correct(pred: str, ref: str, *, kind: str) -> bool:\n",
    "    if kind == 'numeric':\n",
    "        return _normalize_numeric(pred) == _normalize_numeric(ref)\n",
    "    if kind == 'choice':\n",
    "        return _normalize_choice_letter(pred) == _normalize_choice_letter(ref)\n",
    "    return _norm_compact(pred) == _norm_compact(ref)\n",
    "\n",
    "def _require_load_dataset():\n",
    "    try:\n",
    "        from datasets import load_dataset  # type: ignore\n",
    "    except Exception as e:  # noqa: BLE001\n",
    "        raise RuntimeError(\n",
    "            \"Missing dependency: datasets. Install with: %pip install datasets\"\n",
    "        ) from e\n",
    "    return load_dataset\n",
    "\n",
    "def _sample_indices(n_total: int, n_sample: int, seed: int) -> list[int]:\n",
    "    n_sample = min(int(n_sample), int(n_total))\n",
    "    rnd = random.Random(seed)\n",
    "    return rnd.sample(range(n_total), n_sample)\n",
    "\n",
    "def _parse_gsm8k_reference(ans: str) -> str:\n",
    "    # GSM8K answers typically end with '#### <number>'\n",
    "    if '####' in ans:\n",
    "        return ans.split('####')[-1].strip()\n",
    "    return (ans or '').strip()\n",
    "\n",
    "def _parse_math_reference(ans: str) -> str:\n",
    "    # MATH solutions often contain a boxed final answer.\n",
    "    if not ans:\n",
    "        return ''\n",
    "    m = re.findall(r'\\\\boxed\\{([^}]*)\\}', ans)\n",
    "    if m:\n",
    "        return m[-1].strip()\n",
    "    # fallback: also handle \\boxed <...> variants loosely\n",
    "    m2 = re.findall(r'\\\\boxed\\s*([^\\n\\r]+)', ans)\n",
    "    if m2:\n",
    "        return m2[-1].strip()\n",
    "    return ans.strip()\n",
    "\n",
    "def _format_freeform_problem(question: str) -> str:\n",
    "    q = _norm(question)\n",
    "    return q + \"\\n\\nReturn exactly one line: FINAL: <answer>.\"\n",
    "\n",
    "def _format_mc_problem(stem: str, choices: list[tuple[str, str]]) -> str:\n",
    "    lines = [_norm(stem), \"\", \"Choices:\"]\n",
    "    for label, text in choices:\n",
    "        lines.append(f\"{label}. {_norm(text)}\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"Return exactly one line: FINAL: <choice-letter>. Example: FINAL: C\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def _load_math500(load_dataset):\n",
    "    # Prefer a stable MATH-500 dataset id; keep fallbacks for portability.\n",
    "    candidates = [\n",
    "        ('HuggingFaceH4/MATH-500', None),\n",
    "        ('lighteval/MATH-500', None),\n",
    "    ]\n",
    "    last_err = None\n",
    "    for name, subset in candidates:\n",
    "        try:\n",
    "            if subset is None:\n",
    "                return load_dataset(name, split='test')\n",
    "            return load_dataset(name, subset, split='test')\n",
    "        except Exception as e:  # noqa: BLE001\n",
    "            last_err = e\n",
    "    raise RuntimeError(f\"Failed to load MATH500 from {candidates}: {last_err}\")\n",
    "\n",
    "def load_real_benchmarks(*, samples_per_suite: int, seed: int) -> dict[str, list[dict]]:\n",
    "    load_dataset = _require_load_dataset()\n",
    "\n",
    "    # 1) GSM8K (use test split)\n",
    "    gsm = load_dataset('gsm8k', 'main', split='test')\n",
    "    gsm_idx = _sample_indices(len(gsm), samples_per_suite, seed + 1)\n",
    "    gsm_items = []\n",
    "    for i in gsm_idx:\n",
    "        row = gsm[int(i)]\n",
    "        q = row.get('question', '')\n",
    "        ref = _parse_gsm8k_reference(row.get('answer', ''))\n",
    "        gsm_items.append({\n",
    "            'id': f\"gsm8k:test:{int(i)}\",\n",
    "            'problem': _format_freeform_problem(q),\n",
    "            'reference': str(ref),\n",
    "            'kind': 'numeric',\n",
    "        })\n",
    "\n",
    "    # 2) SVAMP (use test split)\n",
    "    sv = load_dataset('svamp', split='test')\n",
    "    sv_idx = _sample_indices(len(sv), samples_per_suite, seed + 2)\n",
    "    sv_items = []\n",
    "    for i in sv_idx:\n",
    "        row = sv[int(i)]\n",
    "        body = row.get('Body', '') or row.get('body', '')\n",
    "        question = row.get('Question', '') or row.get('question', '')\n",
    "        ref = row.get('Answer', None)\n",
    "        if ref is None:\n",
    "            ref = row.get('answer', '')\n",
    "        q_text = (str(body).strip() + \"\\n\" + str(question).strip()).strip()\n",
    "        sv_items.append({\n",
    "            'id': f\"svamp:test:{int(i)}\",\n",
    "            'problem': _format_freeform_problem(q_text),\n",
    "            'reference': str(ref),\n",
    "            'kind': 'numeric',\n",
    "        })\n",
    "\n",
    "    # 3) MATH500 (competition-style math; free-form answer)\n",
    "    math500 = _load_math500(load_dataset)\n",
    "    m_idx = _sample_indices(len(math500), samples_per_suite, seed + 3)\n",
    "    m_items = []\n",
    "    for i in m_idx:\n",
    "        row = math500[int(i)]\n",
    "        # Common fields: 'problem', 'solution', sometimes 'answer'\n",
    "        q = row.get('problem', '') or row.get('question', '')\n",
    "        ref = row.get('answer', None)\n",
    "        if ref is None:\n",
    "            ref = _parse_math_reference(row.get('solution', '') or row.get('final_answer', '') or '')\n",
    "        m_items.append({\n",
    "            'id': f\"math500:test:{int(i)}\",\n",
    "            'problem': _format_freeform_problem(str(q)),\n",
    "            'reference': str(ref),\n",
    "            # Scoring MATH exactly is non-trivial; we use normalized exact match as a strict baseline.\n",
    "            'kind': 'text',\n",
    "        })\n",
    "\n",
    "    # 4) CommonsenseQA (use validation split; multiple choice)\n",
    "    csqa = load_dataset('commonsense_qa', split='validation')\n",
    "    csqa_idx = _sample_indices(len(csqa), samples_per_suite, seed + 4)\n",
    "    csqa_items = []\n",
    "    for i in csqa_idx:\n",
    "        row = csqa[int(i)]\n",
    "        q = row.get('question', '')\n",
    "        ch = row.get('choices', {}) or {}\n",
    "        labels = ch.get('label', []) or []\n",
    "        texts = ch.get('text', []) or []\n",
    "        choices = list(zip(labels, texts, strict=False))\n",
    "        ref = row.get('answerKey', '')\n",
    "        csqa_items.append({\n",
    "            'id': f\"commonsense_qa:val:{int(i)}\",\n",
    "            'problem': _format_mc_problem(q, choices),\n",
    "            'reference': str(ref),\n",
    "            'kind': 'choice',\n",
    "        })\n",
    "\n",
    "    return {\n",
    "        'GSM8K': gsm_items,\n",
    "        'SVAMP': sv_items,\n",
    "        'MATH500': m_items,\n",
    "        'CommonsenseQA': csqa_items,\n",
    "    }\n",
    "\n",
    "BENCHMARKS = load_real_benchmarks(samples_per_suite=SAMPLES_PER_SUITE, seed=RANDOM_SEED)\n",
    "print({k: len(v) for k, v in BENCHMARKS.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97140379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline: single-pass local Transformers inference (no tools, no parallel attempts)\n",
    "BASELINE_SYSTEM = (\n",
    "    'You are a careful reasoner. Do not call tools. '\n",
    "    'Return exactly one line: FINAL: <answer>.'\n",
    ")\n",
    "\n",
    "async def baseline_answer(problem: str) -> str:\n",
    "    messages = [\n",
    "        ChatMessage(role='system', content=BASELINE_SYSTEM),\n",
    "        ChatMessage(role='user', content=problem),\n",
    "    ]\n",
    "    out = await client.chat(messages, temperature=TEMPERATURE, max_tokens=MAX_NEW_TOKENS, n=1)\n",
    "    return extract_final(out[0])\n",
    "\n",
    "async def baseline_run_suite(items):\n",
    "    rows = []\n",
    "    for it in items:\n",
    "        t0 = time.time()\n",
    "        pred = await baseline_answer(it['problem'])\n",
    "        dt = time.time() - t0\n",
    "        rows.append({\n",
    "            'id': it['id'],\n",
    "            'pred': pred,\n",
    "            'ref': it['reference'],\n",
    "            'kind': it.get('kind', 'text'),\n",
    "            'correct': is_correct(pred, it['reference'], kind=it.get('kind', 'text')),\n",
    "            'latency_s': dt,\n",
    "        })\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da540d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VECTRA function-calling: tool loop + parallel attempts + early-stop consensus\n",
    "VECTRA_ATTEMPTS = 4\n",
    "VECTRA_EARLY_STOP = 2\n",
    "VECTRA_MAX_TURNS = 16\n",
    "\n",
    "tool_example = json.dumps({'tool': 'python', 'code': 'print(2+2)'})\n",
    "VECTRA_SYSTEM = (\n",
    "    'You may use an external Python tool. '\n",
    "    f'To call it, output ONLY a JSON object like: {tool_example}. '\n",
    "    'To finish, output: FINAL: <your answer>.'\n",
    ")\n",
    "\n",
    "async def vectra_answer(problem: str) -> str:\n",
    "    cfg = ScenarioSolveConfig(\n",
    "        transformers_client=client,\n",
    "        system_prompt=VECTRA_SYSTEM,\n",
    "        attempts=AttemptConfig(\n",
    "            attempts=VECTRA_ATTEMPTS,\n",
    "            early_stop=VECTRA_EARLY_STOP,\n",
    "            max_concurrency=VECTRA_ATTEMPTS,\n",
    "            tool_loop=ToolLoopConfig(\n",
    "                max_turns=VECTRA_MAX_TURNS,\n",
    "                temperature=TEMPERATURE,\n",
    "            ),\n",
    "        ),\n",
    "        return_trace=False,\n",
    "    )\n",
    "    res = await solve_scenario_async(problem, cfg)\n",
    "    return extract_final('FINAL: ' + res.answer)\n",
    "\n",
    "async def vectra_run_suite(items):\n",
    "    rows = []\n",
    "    for it in items:\n",
    "        t0 = time.time()\n",
    "        pred = await vectra_answer(it['problem'])\n",
    "        dt = time.time() - t0\n",
    "        rows.append({\n",
    "            'id': it['id'],\n",
    "            'pred': pred,\n",
    "            'ref': it['reference'],\n",
    "            'kind': it.get('kind', 'text'),\n",
    "            'correct': is_correct(pred, it['reference'], kind=it.get('kind', 'text')),\n",
    "            'latency_s': dt,\n",
    "        })\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab14972d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(rows):\n",
    "    n = len(rows)\n",
    "    acc = sum(1 for r in rows if r['correct']) / max(1, n)\n",
    "    lat = [r['latency_s'] for r in rows]\n",
    "    if not lat:\n",
    "        return {'n': n, 'accuracy': acc, 'avg_latency_s': 0.0, 'p95_latency_s': 0.0}\n",
    "    lat_sorted = sorted(lat)\n",
    "    p95 = lat_sorted[max(0, math.ceil(0.95 * len(lat_sorted)) - 1)]\n",
    "    return {\n",
    "        'n': n,\n",
    "        'accuracy': acc,\n",
    "        'avg_latency_s': statistics.mean(lat),\n",
    "        'p95_latency_s': p95,\n",
    "    }\n",
    "\n",
    "def print_table(rows):\n",
    "    headers = ['suite', 'mode', 'n', 'accuracy', 'avg_latency_s', 'p95_latency_s']\n",
    "    print('\\t'.join(headers))\n",
    "    for r in rows:\n",
    "        print('\\t'.join([\n",
    "            str(r['suite']),\n",
    "            str(r['mode']),\n",
    "            str(r['n']),\n",
    "            f\"{r['accuracy']:.3f}\",\n",
    "            f\"{r['avg_latency_s']:.3f}\",\n",
    "            f\"{r['p95_latency_s']:.3f}\",\n",
    "        ]))\n",
    "\n",
    "async def run_all():\n",
    "    baseline_all = {}\n",
    "    vectra_all = {}\n",
    "\n",
    "    for suite, items in BENCHMARKS.items():\n",
    "        print(f\"Baseline: {suite}\")\n",
    "        baseline_all[suite] = await baseline_run_suite(items)\n",
    "\n",
    "    for suite, items in BENCHMARKS.items():\n",
    "        print(f\"VECTRA: {suite}\")\n",
    "        vectra_all[suite] = await vectra_run_suite(items)\n",
    "\n",
    "    summary = []\n",
    "    for suite in BENCHMARKS.keys():\n",
    "        summary.append({'suite': suite, 'mode': 'baseline', **summarize(baseline_all[suite])})\n",
    "        summary.append({'suite': suite, 'mode': 'vectra', **summarize(vectra_all[suite])})\n",
    "\n",
    "    print('\\nPer-suite summary')\n",
    "    print('---------------')\n",
    "    print_table(summary)\n",
    "\n",
    "    baseline_flat = [r for v in baseline_all.values() for r in v]\n",
    "    vectra_flat = [r for v in vectra_all.values() for r in v]\n",
    "\n",
    "    agg = [\n",
    "        {'suite': 'ALL', 'mode': 'baseline', **summarize(baseline_flat)},\n",
    "        {'suite': 'ALL', 'mode': 'vectra', **summarize(vectra_flat)},\n",
    "    ]\n",
    "\n",
    "    print('\\nAggregate summary')\n",
    "    print('-----------------')\n",
    "    print_table(agg)\n",
    "\n",
    "    acc_delta = agg[1]['accuracy'] - agg[0]['accuracy']\n",
    "    lat_delta = agg[1]['avg_latency_s'] - agg[0]['avg_latency_s']\n",
    "\n",
    "    print('\\nImpact (VECTRA - Baseline)')\n",
    "    print('--------------------------')\n",
    "    print('Accuracy delta:', f\"{acc_delta:+.3f}\")\n",
    "    print('Avg latency delta (s):', f\"{lat_delta:+.3f}\")\n",
    "\n",
    "    print('\\nPaste-ready summary')\n",
    "    print('------------------')\n",
    "    print(\n",
    "        f\"Model: {MODEL_ID} (Transformers local inference)\\n\"\n",
    "        f\"Benchmarks: GSM8K, SVAMP, MATH500, CommonsenseQA (sampled subsets).\\n\"\n",
    "        f\"Scope: {agg[0]['n']} questions total (SAMPLES_PER_SUITE={SAMPLES_PER_SUITE}).\\n\"\n",
    "        f\"Baseline (single-pass): accuracy={agg[0]['accuracy']:.3f}, avg latency={agg[0]['avg_latency_s']:.3f}s.\\n\"\n",
    "        f\"VECTRA (tools + parallel attempts): accuracy={agg[1]['accuracy']:.3f}, avg latency={agg[1]['avg_latency_s']:.3f}s.\\n\"\n",
    "        f\"Net impact: accuracy delta={acc_delta:+.3f}, latency delta={lat_delta:+.3f}s.\\n\"\n",
    "        \"\",\n",
    "        'Method notes:\\n',\n",
    "        '- Baseline uses one local generation per question.\\n',\n",
    "        '- VECTRA runs multiple attempts and can execute Python when requested via a strict JSON tool protocol.\\n',\n",
    "        '- Early-stop consensus can reduce extra attempts when answers match.\\n',\n",
    "        '- MATH500 scoring here uses strict normalized string match; for papers, consider a domain-aware grader.\\n',\n",
    "    )\n",
    "\n",
    "await run_all()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
